{"cells":[{"cell_type":"markdown","id":"8cfa8394","metadata":{},"source":["# PySpark notebook \n","## from the gcloud dataproc cluster's web interface JupyterLab."]},{"cell_type":"markdown","id":"dec21b0d","metadata":{},"source":["<img src=\"dataproc_jupyterLab_interface.png\" width=\"800\" />"]},{"cell_type":"code","execution_count":1,"id":"23c27e1e-3f3f-43f1-9e9b-3da09f34d395","metadata":{},"outputs":[],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col\n","from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, FloatType, TimestampType"]},{"cell_type":"code","execution_count":null,"id":"a83ea4cf-944f-47b1-829d-32a9eff03a6d","metadata":{},"outputs":[],"source":["# Initialize Spark Session\n","spark = SparkSession.builder \\\n","    .appName(\"JsonToBigQuery\") \\\n","    .getOrCreate()"]},{"cell_type":"code","execution_count":4,"id":"6be159c8-1b3a-40cf-b2c2-82966f6c66e7","metadata":{},"outputs":[{"data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - hive</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://cluster-vlille-m.us-central1-c.c.zapart-data-vlille.internal:41177\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.3.2</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>yarn</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>PySparkShell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x7f53ca6dc940>"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["spark"]},{"cell_type":"code","execution_count":5,"id":"545c5992-8d56-4d8f-9b83-d18df2b44532","metadata":{},"outputs":[],"source":["# Define JSON schema\n","json_schema = StructType([\n","    StructField(\"nhits\", IntegerType()),\n","    StructField(\"parameters\", StructType([\n","        StructField(\"dataset\", StringType()),\n","        StructField(\"rows\", IntegerType()),\n","        StructField(\"start\", IntegerType()),\n","        StructField(\"format\", StringType()),\n","        StructField(\"timezone\", StringType())\n","    ])),\n","    StructField(\"records\", ArrayType(StructType([\n","        StructField(\"datasetid\", StringType()),\n","        StructField(\"recordid\", StringType()),\n","        StructField(\"fields\", StructType([\n","            StructField(\"nbvelosdispo\", IntegerType()),\n","            StructField(\"nbplacesdispo\", IntegerType()),\n","            StructField(\"libelle\", StringType()),\n","            StructField(\"adresse\", StringType()),\n","            StructField(\"nom\", StringType()),\n","            StructField(\"etat\", StringType()),\n","            StructField(\"commune\", StringType()),\n","            StructField(\"etatconnexion\", StringType()),\n","            StructField(\"type\", StringType()),\n","            StructField(\"geo\", ArrayType(FloatType())),\n","            StructField(\"localisation\", ArrayType(FloatType())),\n","            StructField(\"datemiseajour\", TimestampType())\n","        ])),\n","        StructField(\"geometry\", StructType([\n","            StructField(\"type\", StringType()),\n","            StructField(\"coordinates\", ArrayType(FloatType()))\n","        ])),\n","        StructField(\"record_timestamp\", TimestampType())\n","    ])))\n","])"]},{"cell_type":"code","execution_count":6,"id":"e49ce4d1-a62b-423c-8f73-708999c39d9a","metadata":{},"outputs":[],"source":["import subprocess\n","\n","def list_files_in_bucket(bucket_name):\n","    # Run the gsutil ls command and capture the output\n","    command = f\"gsutil ls gs://{bucket_name}\"\n","    try:\n","        # Run the command and capture the output as a byte string\n","        output = subprocess.check_output(command, shell=True)\n","        \n","        # Decode the byte string to a regular string and split it into lines\n","        file_paths = output.decode(\"utf-8\").strip().split(\"\\n\")\n","        \n","        # Return the list of file paths\n","        return file_paths\n","    except subprocess.CalledProcessError as e:\n","        # Handle any errors that occurred during the command execution\n","        print(f\"Error: {e}\")\n","        return []"]},{"cell_type":"code","execution_count":10,"id":"b7d91d52-33dd-4094-bd44-76cf9a10cb9b","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["91549 files, to insert 26457661 rows in bigquery\n"]}],"source":["# Specify your bucket name\n","bucket_name = \"vlille_data_json\"\n","\n","# Call the function to get the list of files in the bucket\n","file_paths = list_files_in_bucket(bucket_name)\n","\n","# Print the list of file paths\n","# print(\"List of files in the bucket:\")\n","n_files = len(file_paths)\n","print(n_files, 'files, to insert', n_files*289, 'rows in bigquery')"]},{"cell_type":"code","execution_count":11,"id":"262a0797-cd73-49f8-a9d9-422e424051bf","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# Read JSON data from Google Cloud Storage\n","json_data = spark.read.schema(json_schema).json(file_paths)"]},{"cell_type":"code","execution_count":12,"id":"0beac306-fb5f-4dbf-9767-4676a6eda75d","metadata":{},"outputs":[{"data":{"text/plain":["DataFrame[nhits: int, parameters: struct<dataset:string,rows:int,start:int,format:string,timezone:string>, records: array<struct<datasetid:string,recordid:string,fields:struct<nbvelosdispo:int,nbplacesdispo:int,libelle:string,adresse:string,nom:string,etat:string,commune:string,etatconnexion:string,type:string,geo:array<float>,localisation:array<float>,datemiseajour:timestamp>,geometry:struct<type:string,coordinates:array<float>>,record_timestamp:timestamp>>]"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["json_data"]},{"cell_type":"code","execution_count":13,"id":"335d002d-63ea-4dc0-8c39-eb8a78afea46","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# Flatten the nested JSON structure\n","flattened_data = json_data.select(col(\"records.fields.nbvelosdispo\").alias(\"nb_available_bikes\"),\n","                                  col(\"records.fields.nbplacesdispo\").alias(\"nb_available_places\"),\n","                                  col(\"records.fields.libelle\").alias(\"station_id\"),\n","                                  col(\"records.fields.etat\").alias(\"operational_state\"),\n","                                  col(\"records.fields.etatconnexion\").alias(\"connexion\"),\n","                                  col(\"records.fields.datemiseajour\").alias(\"datemiseajour\"),\n","                                  col(\"records.record_timestamp\").alias(\"record_timestamp\"))\n","\n","# Write data to BigQuery\n","flattened_data.write \\\n","    .format(\"bigquery\") \\\n","    .mode(\"overwrite\") \\\n","    .option(\"temporaryGcsBucket\", \"dataproc_test_yzpt\") \\\n","    .option(\"parentProject\", \"zapart-data-vlille\") \\\n","    .option(\"table\", \"zapart-data-vlille.vlille_dataset.dataproc_test\") \\\n","    .save()"]},{"cell_type":"code","execution_count":null,"id":"fe7db4ec-7276-46cc-959d-cf9ccaf7ae76","metadata":{},"outputs":[],"source":["# duration : 20min @ 22k rows/s"]},{"cell_type":"code","execution_count":null,"id":"3fa3cd57-b81b-4e2b-8819-f57e47bd9a11","metadata":{},"outputs":[],"source":["# Stop the Spark session\n","spark.stop()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":5}
